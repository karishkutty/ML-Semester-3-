# -*- coding: utf-8 -*-
"""MLpracticals.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11TC4TWEZWC3pR5zdlRzA31HbxAHW3TuX
"""

#Design a simple machine learning model to train the training instances and test the same.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.datasets import load_iris

iris=load_iris()
iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
target_df = pd.DataFrame(data=iris.target, columns=['species'])
iris.feature_names
def converter(species):
  if species == 0:
    return 'setosa'
  elif species == 1:
    return 'versicolor'
  else:return 'virginica'
target_df['species'] = target_df['species'].apply(converter)
iris_df = pd.concat([iris_df, target_df], axis=1)
iris_df.describe()
plt.style.use('ggplot')
sns.pairplot(iris_df, hue='species')

# Problem : Predict Sepal length (cm) iris_df.drop('species', axis=1, inplace=True)
target_df = pd.DataFrame(columns=['species'], data=iris.target)
iris_df = pd.concat([iris_df, target_df], axis=1)
X=iris_df.drop(labels='sepal length (cm)', axis=1)
y = iris_df['sepal length (cm)']
Xtrain, Xtest, ytrain, ytest = train_test_split(X,y,test_size=0.3, random_state=101)
lr = LinearRegression()
lr.fit(Xtrain, ytrain)
pred = lr.predict(Xtest)
print('Mean Absolute Error : ',mean_absolute_error(ytest, pred))

print('Mean Squared Error : ',mean_squared_error(ytest, pred))
print('Mean Root Squared Error : ',np.sqrt(mean_squared_error(ytest, pred)))
iris_df.iloc[6]
d={'sepal length (cm)':[4.6],
'sepal width (cm)':[3.4],
'petal length (cm)':[1.4],
'petal width (cm)':[0.3], 'species':0}
test_df = pd.DataFrame(data=d)
test_df
Xtest = test_df.drop(labels='sepal length (cm)', axis=1)
ytest = test_df['sepal length (cm)']
pred = lr.predict(Xtest)
pred, test_df['sepal length (cm)']

#Practical No. 1-B
#Aim: Implement and demonstrate the FIND-S algorithm for finding the most specific hypothesis based on a given set of training data samples. Read the training data from a .CSV file.
#Code:

import pandas as pd
import numpy as np
data = pd.read_csv("/content/tennis.csv")
print("Dataset : \n", data)
d = np.array(data)[:, :-1]
print("The attributes are: ", d)
target = np.array(data)[:, -1]
print("The target is: ", target)
def train(c, t):
    specific_hypothesis = None
    for i, val in enumerate(t):
        if val == "Yes":
            specific_hypothesis = c[i].copy()
        break
        if specific_hypothesis is None:
         print("No positive instance found!")
        return None
    for i, val in enumerate(c):
        if t[i] == "Yes":
            for x in range(len(specific_hypothesis)):
                if val[x] != specific_hypothesis[x]:
                    specific_hypothesis[x] = '?'
                else: pass
    return specific_hypothesis
final_hypothesis = train(d, target)
if final_hypothesis is not None:
    print("The final hypothesis is:", final_hypothesis)

#Aim: Perform Data Loading, Feature selection (Principal Component analysis) and Feature Scoring and Ranking.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

plt.style.use('ggplot')

# Load iris dataset
irisdata = load_iris()
X = irisdata.data
y = irisdata.target

# Standardize the features
scaler = StandardScaler()
scaler.fit(X)
X = scaler.transform(X)

# Apply PCA
pca = PCA(n_components=2)
X_new = pca.fit_transform(X)

# Plotting
fig, axes = plt.subplots(1, 2)

# Before PCA
axes[0].scatter(X[:, 0], X[:, 1], c=y)
axes[0].set_xlabel('Feature 1')
axes[0].set_ylabel('Feature 2')
axes[0].set_title('Before PCA')

# After PCA
axes[1].scatter(X_new[:, 0], X_new[:, 1], c=y)
axes[1].set_xlabel('Principal Component 1')
axes[1].set_ylabel('Principal Component 2')
axes[1].set_title('After PCA')

plt.tight_layout()
plt.show()

#Aim: For a given set of training data examples stored in a .CSV file, implement and demonstrate the Candidate-Elimination algorithm to output a description of the set of all hypotheses consistent with the training examples.

import numpy as np
import pandas as pd

dataset = pd.read_csv('/content/tennis.csv')
features = np.array(dataset.iloc[:, 0:-1])
print("Instances : ", features)

target = np.array(dataset.iloc[:, -1])
print("Target values : ", target)

def learn(features, target):
    specific_hypo = features[0].copy()
    print("Specific Hypothesis : ", specific_hypo)
    general_hypo = [["?" for _ in range(len(specific_hypo))] for _ in range(len(specific_hypo))]
    print("General Hypothesis : ", general_hypo)

    for i, h in enumerate(features):
        print("Instance ", i+1, " is ", h)
        if target[i] == "yes":
            print("Positive Instance")
            for x in range(len(specific_hypo)):
                if h[x] != specific_hypo[x]:
                    specific_hypo[x] = "?"
                    general_hypo[x][x] = "?"

        if target[i] == "no":
            print("Negative Instance")
            for x in range(len(specific_hypo)):
                if h[x] != specific_hypo[x]:
                    general_hypo[x][x] = specific_hypo[x]
                else:
                    general_hypo[x][x] = "?"

        print("Specific boundary after ", i+1, " instance is ", specific_hypo)
        print("General boundary after ", i+1, " instance is ", general_hypo)

    general_hypo = [h for h in general_hypo if '?' not in h]
    return specific_hypo, general_hypo
s_final, g_final = learn(features, target)
print("Final Specific_h: ", s_final, sep="\n")
print("Final General_h: ", g_final, sep="\n")

#Aim: Write a program to implement the na√Øve Bayesian classifier for a sample training data set stored as a .CSV file. Compute the accuracy of the classifier, considering few test data sets.

import csv
import random
import math

def loadCsv(filename):
    with open(filename, "r") as file:
        lines = csv.reader(file)
        dataset = list(lines)[1:]  # Skip the first row (headers)
        for i in range(len(dataset)):
            # Converting strings into numbers for processing
            dataset[i] = [float(x) for x in dataset[i]]
    return dataset

def splitDataset(dataset, splitRatio):
    # 67% training size
    trainSize = int(len(dataset) * splitRatio)
    trainSet = []
    copy = list(dataset)
    while len(trainSet) < trainSize:
        # generate indices for the dataset list randomly to pick elements for training data
        index = random.randrange(len(copy))
        trainSet.append(copy.pop(index))
    return [trainSet, copy]

def separateByClass(dataset):
    separated = {}
    # creates a dictionary of classes 1 and 0 where the values are the instances belonging to each class
    for i in range(len(dataset)):
        vector = dataset[i]
        if vector[-1] not in separated:
            separated[vector[-1]] = []
        separated[vector[-1]].append(vector)
    return separated

def mean(numbers):
    return sum(numbers) / float(len(numbers))

def stdev(numbers):
    avg = mean(numbers)
    variance = sum([pow(x - avg, 2) for x in numbers]) / float(len(numbers) - 1)
    return math.sqrt(variance)

def summarize(dataset):
    summaries = [(mean(attribute), stdev(attribute)) for attribute in zip(*dataset)]
    del summaries[-1]
    return summaries

def summarizeByClass(dataset):
    separated = separateByClass(dataset)
    summaries = {}
    for classValue, instances in separated.items():
        # summaries is a dictionary of tuples(mean, std) for each class value
        summaries[classValue] = summarize(instances)
    return summaries

def calculateProbability(x, mean, stdev):
    exponent = math.exp(-(math.pow(x - mean, 2) / (2 * math.pow(stdev, 2))))
    return (1 / (math.sqrt(2 * math.pi) * stdev)) * exponent

def calculateClassProbabilities(summaries, inputVector):
    probabilities = {}
    for classValue, classSummaries in summaries.items():
        # class and attribute information as mean and sd
        probabilities[classValue] = 1
        for i in range(len(classSummaries)):
            mean, stdev = classSummaries[i]  # take mean and sd of every attribute for class 0 and 1 separately
            x = inputVector[i]  # test vector's attribute
            # use normal distribution
            probabilities[classValue] *= calculateProbability(x, mean, stdev)
    return probabilities

def predict(summaries, inputVector):
    probabilities = calculateClassProbabilities(summaries, inputVector)
    bestLabel, bestProb = None, -1
    for classValue, probability in probabilities.items():
        # assigns that class which has the highest probability
        if bestLabel is None or probability > bestProb:
            bestProb = probability
            bestLabel = classValue
    return bestLabel

def getPredictions(summaries, testSet):
    predictions = []
    for i in range(len(testSet)):
        result = predict(summaries, testSet[i])
        predictions.append(result)
    return predictions

def getAccuracy(testSet, predictions):
    correct = 0
    for i in range(len(testSet)):
        if testSet[i][-1] == predictions[i]:
            correct += 1
    return (correct / float(len(testSet))) * 100.0

def main():
    filename = 'diabetes.csv'
    splitRatio = 0.67
    dataset = loadCsv(filename)
    trainingSet, testSet = splitDataset(dataset, splitRatio)
    print('Split {0} rows into train={1} and test={2} rows'.format(len(dataset), len(trainingSet), len(testSet)))
    # prepare model
    summaries = summarizeByClass(trainingSet)
    # test model
    predictions = getPredictions(summaries, testSet)
    accuracy = getAccuracy(testSet, predictions)
    print('Accuracy of the classifier is : {0}%'.format(accuracy))

main()

#Aim: Write a program to implement Decision Tree and Random forest with Prediction, Test Score and Confusion Matrix.
import numpy as np
import pandas as pd
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split

def train_using_gini(xtrain, ytrain):
    gini_classifier = DecisionTreeClassifier(criterion='gini', random_state=100, max_depth=3, min_samples_leaf=5)
    gini_classifier.fit(xtrain, ytrain)
    return gini_classifier

def train_using_entropy(xtrain, ytrain):
    entropy_classifier = DecisionTreeClassifier(criterion='entropy', random_state=100, max_depth=3, min_samples_leaf=5)
    entropy_classifier.fit(xtrain, ytrain)
    return entropy_classifier

def prediction(xtest, classifier):
    ypred = classifier.predict(xtest)
    return ypred

def calculate_accuracy(ytest, ypred):
    cm = confusion_matrix(ytest, ypred)
    accuracy = accuracy_score(ytest, ypred) * 100
    report = classification_report(ytest, ypred)
    return cm, accuracy, report

balance_data = pd.read_csv('diabetes.csv')
print("Shape of the data : ", balance_data.shape)

X = balance_data.iloc[:, 1:5]
Y = balance_data.iloc[:, 0]
xtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size=0.3, random_state=100)

gini_cls = train_using_gini(xtrain, ytrain)
entropy_cls = train_using_entropy(xtrain, ytrain)

print("Results using gini index")
pred_gini = prediction(xtest, gini_cls)
cm_gini, accuracy_gini, report_gini = calculate_accuracy(ytest, pred_gini)
print("Confusion Matrix : \n", cm_gini)
print("Accuracy Score : ", accuracy_gini)
print("Classification Report : \n", report_gini)



print("\nResults using entropy")
pred_entropy = prediction(xtest, entropy_cls)
cm_entropy, accuracy_entropy, report_entropy = calculate_accuracy(ytest, pred_entropy)
print("Confusion Matrix : \n", cm_entropy)
print("Accuracy Score : ", accuracy_entropy)
print("Classification Report : \n", report_entropy)

#Aim: For a given set of training data examples stored in a .CSV file implement Least Square Regression algorithm.

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

data = pd.read_csv('headbrain.csv')
print("Shape:", data.shape)
data.head()

X = data['Head Size(cm^3)'].values
Y = data['Brain Weight(grams)'].values

xmean = np.mean(X)
ymean = np.mean(Y)
n = len(X)
print("X mean: {0}\tY mean: {1}\tTotal number of values: {2}".format(xmean, ymean, n))

numerator = 0
denominator = 0
for i in range(n):
    numerator += (X[i] - xmean) * (Y[i] - ymean)
    denominator += (X[i] - xmean) ** 2

m = numerator / denominator
c = ymean - (m * xmean)
print("Slope: {0},\tIntercept: {1}".format(m, c))

xmax = np.max(X) + 100
xmin = np.min(X) - 100

x = np.linspace(xmin, xmax, 1000)
y = m * x + c

plt.plot(x, y, color='#58b970', label='Regression Line')
plt.scatter(X, Y, c='#ef5423', label='Scatter plot')
plt.xlabel('Head Size(cm^3)')
plt.ylabel('Brain Weight(grams)')
plt.legend()
plt.show()

# Calculating the Root Mean Squared Error
rmse = 0
for i in range(n):
    ypred = m * X[i] + c
    rmse += (Y[i] - ypred) ** 2

rmse = np.sqrt(rmse / n)
print("Root Mean Squared Error (RMSE):", rmse)

# Calculating R2 Score
tot_sum_of_sqr = 0
tot_sum_of_sqr_res = 0
for i in range(n):
    ypred = m * X[i] + c
    tot_sum_of_sqr += (Y[i] - ymean) ** 2
    tot_sum_of_sqr_res += (Y[i] - ypred) ** 2

r2 = 1 - (tot_sum_of_sqr_res / tot_sum_of_sqr)
print("R2 score:", r2)

#Aim: For a given set of training data examples stored in a .CSV file implement Logistic Regression algorithm.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

X, y = make_classification(n_features=4)
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.1)

def standardize(X):
    for i in range(np.shape(X)[1]):
        X[:,i] = (X[:,i] - np.mean(X[:,i])) / np.std(X[:,i])
    return X

def f1_score(y, y_hat):
    tp, tn, fp, fn = 0,0,0,0
    for i in range(len(y)):
        if(y[i] == 1 and y_hat[i] == 1): tp += 1
        elif(y[i] == 1 and y_hat[i] == 0): fn += 1
        elif(y[i] == 0 and y_hat[i] == 1): fp += 1
        elif(y[i] == 0 and y_hat[i] == 0): tn += 1
    precision = tp / (tp + fp)
    recall = tp / (tp + fn)
    f1score = 2 * (precision * recall) / (precision + recall)
    return f1score

class LogisticRegression:
    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    def initialize(self, X):
        weights = np.zeros((np.shape(X)[1]+1, 1))
        X = np.c_[np.ones((np.shape(X)[0], 1)), X]
        return weights, X

    def fit(self, X, y, alpha=0.001, num_iter=400):
        weights, X = self.initialize(X)
        for i in range(num_iter):
            z = np.dot(X, weights)
            gradient = np.dot(X.T, self.sigmoid(z) - np.reshape(y, (len(y), 1))) / len(y)
            weights -= alpha * gradient
        self.weights = weights
        return weights

    def predict(self, X):
        X = np.c_[np.ones((np.shape(X)[0], 1)), X]
        z = np.dot(X, self.weights)
        y_pred = [1 if i > 0.5 else 0 for i in self.sigmoid(z)]
        return y_pred

Xtrain = standardize(Xtrain)
Xtest = standardize(Xtest)

obj = LogisticRegression()
model = obj.fit(Xtrain, ytrain)
ypred = obj.predict(Xtest)
y_train = obj.predict(Xtrain)

print("F1 score for training data:", f1_score(ytrain, y_train))
print("F1 score for testing data:", f1_score(ytest, ypred))

print("Confusion Matrix:")
print(confusion_matrix(ytest, ypred))

#Aim: Write a program to demonstrate the working of the decision tree based ID3 algorithm. Use an appropriate data set for building the decision tree and apply this knowledge to classify a new sample.

import pandas as pd
import math
import numpy as np

data = pd.read_csv("PlayTennis.csv")
features = [feat for feat in data]
features.remove("play")

print("Columns in dataset:", data.columns)  # Debug print to check columns

class Node:
    def __init__(self):
        self.children = []
        self.value = ""
        self.isLeaf = False
        self.pred = ""

def entropy(examples):
    pos = 0.0
    neg = 0.0
    for _, row in examples.iterrows():
        if row["play"] == "Yes":
            pos += 1
        else:
            neg += 1
    if pos == 0.0 or neg == 0.0:
        return 0.0
    else:
        p = pos / (pos + neg)
        n = neg / (pos + neg)
        return -(p * math.log(p, 2) + n * math.log(n, 2))

def info_gain(examples, attr):
    uniq = np.unique(examples[attr])
    gain = entropy(examples)
    for u in uniq:
        subdata = examples[examples[attr] == u]
        sub_e = entropy(subdata)
        gain -= (float(len(subdata)) / float(len(examples))) * sub_e
    return gain

def ID3(examples, attrs):
    root = Node()
    max_gain = 0
    max_feat = ""
    for feature in attrs:
        gain = info_gain(examples, feature)
        if gain > max_gain:
            max_gain = gain
            max_feat = feature
    if max_feat == "":
        # Handle the case where no attribute produces positive gain
        # For example, predict the majority class
        root.isLeaf = True
        root.pred = examples["play"].mode().iloc[0]
        return root
    root.value = max_feat
    uniq = np.unique(examples[max_feat])
    for u in uniq:
        subdata = examples[examples[max_feat] == u]
        if entropy(subdata) == 0.0:
            newNode = Node()
            newNode.isLeaf = True
            newNode.value = u
            newNode.pred = np.unique(subdata["play"])
            root.children.append(newNode)
        else:
            dummyNode = Node()
            dummyNode.value = u
            new_attrs = attrs.copy()
            new_attrs.remove(max_feat)
            child = ID3(subdata, new_attrs)
            dummyNode.children.append(child)
            root.children.append(dummyNode)
    return root

def printTree(root: Node, depth=0):
    for i in range(depth):
        print("\t", end="")
    print(root.value, end="")
    if root.isLeaf:
        print(" -> ", root.pred)
    print()
    for child in root.children:
        printTree(child, depth + 1)

def classify(root: Node, new):
    for child in root.children:
        if child.value == new[root.value]:
            if child.isLeaf:
                print("Predicted Label for new example", new, " is:", child.pred)
                return
            else:
                classify(child.children[0], new)




root = ID3(data, features)
print("Decision Tree is:")
printTree(root)

new = {"outlook": "sunny", "temp": "hot", "humidity": "normal", "wind": "strong"}
classify(root, new)

#Aim: Write a program to implement k-Nearest Neighbor algorithm to classify the iris data set.


import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split  # Separate import statement
from sklearn.neighbors import KNeighborsClassifier

iris = load_iris()
iris.feature_names

df = pd.DataFrame(iris.data, columns=iris.feature_names)
df['target'] = iris.target
df['flower_name'] = df.target.apply(lambda x: iris.target_names[x])

df0 = df[0:50]
df1 = df[50:100]
df2 = df[100:]

plt.xlabel("Sepal Length")
plt.ylabel("Sepal Width")
plt.scatter(df0['sepal length (cm)'], df0['sepal width (cm)'], color="green", marker="+")
plt.scatter(df1['sepal length (cm)'], df1['sepal width (cm)'], color="blue", marker=".")

plt.xlabel("Petal Length")
plt.ylabel("Petal Width")
plt.scatter(df0['petal length (cm)'], df0['petal width (cm)'], color="green", marker="+")
plt.scatter(df1['petal length (cm)'], df1['petal width (cm)'], color="blue", marker=".")

X = df.drop(['target', 'flower_name'], axis='columns')
y = df.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

knn = KNeighborsClassifier(n_neighbors=10)
knn.fit(X_train, y_train)
knn.score(X_test, y_test)

#Aim: Implement the different Distance methods (Euclidean) with Prediction, Test Score and Confusion Matrix.

from scipy.spatial import distance
# defining the points
point_1 = (1, 2, 3)
point_2 = (4, 5, 6)

# computing the Manhattan distance
manhattan_distance = distance.cityblock(point_1, point_2)
print('Manhattan Distance b/w', point_1, 'and', point_2, 'is:', manhattan_distance)

# computing the Minkowski distance
minkowski_distance = distance.minkowski(point_1, point_2, p=3)
print('Minkowski Distance b/w', point_1, 'and', point_2, 'is:', minkowski_distance)

# computing the Minkowski distance of order 1 and Manhattan distance
minkowski_distance_order_1 = distance.minkowski(point_1, point_2, p=1)
print('Minkowski Distance of order 1:', minkowski_distance_order_1, '\nManhattan Distance:', manhattan_distance)

# computing the Euclidean distance
euclidean_distance = distance.euclidean(point_1, point_2)
print('Euclidean Distance b/w', point_1, 'and', point_2, 'is:', euclidean_distance)

# computing the Minkowski distance of order 2 and Euclidean distance
minkowski_distance_order_2 = distance.minkowski(point_1, point_2, p=2)
print('Minkowski Distance of order 2:', minkowski_distance_order_2, '\nEuclidean Distance:', euclidean_distance)

# defining two strings
string_1 = 'euclidean'
string_2 = 'manhattan'

# computing the Hamming distance
hamming_distance = distance.hamming(list(string_1), list(string_2)) * len(string_1)
print('Hamming Distance b/w', string_1, 'and', string_2, 'is:', hamming_distance)

#Aim: Implement the classification model using clustering for the following techniques with K means clustering with Prediction, Test Score and Confusion Matrix.

from sklearn.cluster import KMeans
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from matplotlib import pyplot as plt

# Load the dataset
df = pd.read_csv('income.csv')

# Visualize the data
plt.scatter(df['Age'], df['Income($)'])
plt.xlabel('Age')
plt.ylabel('Income($)')
plt.show()

# KMeans clustering without normalization
km = KMeans(n_clusters=3)
predicted = km.fit_predict(df[['Age', 'Income($)']])
df['cluster'] = predicted

# Visualization
df1 = df[df.cluster == 0]
df2 = df[df.cluster == 1]
df3 = df[df.cluster == 2]
plt.scatter(df1['Age'], df1['Income($)'], color='green')
plt.scatter(df2['Age'], df2['Income($)'], color='red')
plt.scatter(df3['Age'], df3['Income($)'], color='blue')
plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], color='purple', marker='*', label='Centroid')
plt.xlabel('Age')
plt.ylabel('Income($)')
plt.legend()
plt.show()

# Data normalization
scaler = MinMaxScaler()
scaler.fit(df[['Income($)']])
df['Income($)'] = scaler.transform(df[['Income($)']])

scaler.fit(df[['Age']])
df['Age'] = scaler.transform(df[['Age']])

# KMeans clustering after normalization
km = KMeans(n_clusters=3)
predicted = km.fit_predict(df[['Age', 'Income($)']])
df['cluster'] = predicted

# Visualization after normalization
df1 = df[df.cluster == 0]
df2 = df[df.cluster == 1]
df3 = df[df.cluster == 2]
plt.scatter(df1['Age'], df1['Income($)'], color='green')
plt.scatter(df2['Age'], df2['Income($)'], color='red')
plt.scatter(df3['Age'], df3['Income($)'], color='blue')
plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], color='purple', marker='*', label='Centroid')
plt.xlabel('Age')
plt.ylabel('Income($)')
plt.legend()
plt.show()

# Elbow Plot (For checking)
sse = []
k_range = range(1, 10)
for k in k_range:
    km = KMeans(n_clusters=k)
    km.fit(df[['Age', 'Income($)']])
    # Calculating the distance between centroids and the nearest point
    sse.append(km.inertia_)

plt.xlabel('K')
plt.ylabel('Sum of Squared error')
plt.plot(k_range, sse)
plt.show()

# Commented out IPython magic to ensure Python compatibility.
#Aim: Implement the classification model using clustering for the following techniques with hierarchical clustering with Prediction, Test Score and Confusion Matrix.


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import normalize
import scipy.cluster.hierarchy as hc
from sklearn.cluster import AgglomerativeClustering

# %matplotlib inline

data = pd.read_csv('Wholesale customers data.csv')
print("Dimension of the dataset:", data.shape)
data.head()

scaled_data = normalize(data)
scaled_data = pd.DataFrame(scaled_data, columns=data.columns)
scaled_data.head()

plt.figure(figsize=(10, 7))
plt.title("Dendrogram")
dendogram = hc.dendrogram(hc.linkage(scaled_data, method='ward'))

cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')
cluster.fit_predict(scaled_data)

plt.figure(figsize=(10, 7))
plt.scatter(scaled_data['Milk'], scaled_data['Grocery'], c=cluster.labels_)
plt.xlabel('Milk')
plt.ylabel('Grocery')
plt.show()

pip install efficient-apriori

#Aim: Implement the Rule based method and test the same.
#pip install efficient-apriori need to install this first


from efficient_apriori import apriori
import pandas as pd

df = pd.read_csv('GroceryStoreDataSet.csv', names=['products'], header=None)
data = list(df["products"].apply(lambda x: x.split(',')))
transactions = [tuple(i) for i in data]

min_support = 3.5 / len(transactions)
print("Min support:", min_support)
min_confidence = 0.5

itemsets, rules = apriori(transactions, min_support=min_support, min_confidence=min_confidence)

for rule in rules:
    print(rule)

pip install pgmpy

#Aim: Write a program to construct a Bayesian network considering medical data. Use this model to demonstrate the diagnosis of heart patients using standard Heart Disease Data Set.
# pip install pgmpy
import numpy as np
import pandas as pd
from pgmpy.models import BayesianModel
from pgmpy.estimators import MaximumLikelihoodEstimator
from pgmpy.inference import VariableElimination

# Load the Heart Disease Data Set
heartDataset = pd.read_csv('heart.csv')
heartDataset = heartDataset.replace('?', np.nan)

# Define the structure of the Bayesian Network
model = BayesianModel([
    ('age', 'trestbps'), ('age', 'fbs'), ('gender', 'trestbps'),
    ('exang', 'trestbps'), ('trestbps', 'target'), ('fbs', 'target'),
    ('target', 'restecg'), ('target', 'thalach'), ('target', 'chol')
])

# Learn CPDs (Conditional Probability Distributions) using Maximum Likelihood Estimator
print('Learning CPD using Maximum likelihood estimator')
model.fit(heartDataset, estimator=MaximumLikelihoodEstimator)

# Perform inference with Bayesian Network
heart_infer = VariableElimination(model)

# Query probabilities
print('Inferencing with Bayesian Network')
print('Probability of Heart Disease given age=29')
q = heart_infer.query(variables=['target'], evidence={'age': 29})
print(q)

print('Probability of Heart Disease given cholestrol=131')
q = heart_infer.query(variables=['target'], evidence={'chol': 131})
print(q)

#Aim: Implement the non-parametric Locally Weighted Regression algorithm in order to fit data points. Select appropriate data set for your experiment and draw graphs.

import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
X = np.linspace(-3, 3, 100)
X += np.random.normal(scale=0.05, size=100)
y = np.log(np.abs((X**2) - 1) + 0.5)

# Plot the data
plt.scatter(X, y, alpha=0.5)
plt.xlabel('X')
plt.ylabel('y')
plt.title('Scatter Plot of Synthetic Data')
plt.show()

#Aim: Build an Artificial Neural Network by implementing the Back propagation algorithm and test the same using appropriate data sets.

import numpy as np

# Define the sigmoid function
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Define the derivative of sigmoid function
def derivative_sigmoid(x):
    return x * (1 - x)

# Define the neural network class
class NeuralNetwork:
    def __init__(self, input_neurons, hidden_neurons, output_neurons):
        self.input_neurons = input_neurons
        self.hidden_neurons = hidden_neurons
        self.output_neurons = output_neurons

        # Initialize weights and biases
        self.weights_input_hidden = np.random.uniform(size=(self.input_neurons, self.hidden_neurons))
        self.biases_hidden = np.random.uniform(size=(1, self.hidden_neurons))
        self.weights_hidden_output = np.random.uniform(size=(self.hidden_neurons, self.output_neurons))
        self.biases_output = np.random.uniform(size=(1, self.output_neurons))

    # Define the feedforward function
    def feedforward(self, X):
        self.hidden_sum = np.dot(X, self.weights_input_hidden) + self.biases_hidden
        self.hidden_activation = sigmoid(self.hidden_sum)
        self.output_sum = np.dot(self.hidden_activation, self.weights_hidden_output) + self.biases_output
        self.output_activation = sigmoid(self.output_sum)
        return self.output_activation

    # Define the backpropagation function
    def backpropagation(self, X, y, output_activation):
        # Calculate the error
        self.error = y - output_activation
        # Compute gradients
        self.output_delta = self.error * derivative_sigmoid(output_activation)
        self.hidden_error = self.output_delta.dot(self.weights_hidden_output.T)
        self.hidden_delta = self.hidden_error * derivative_sigmoid(self.hidden_activation)
        # Update weights and biases
        self.weights_hidden_output += self.hidden_activation.T.dot(self.output_delta) * learning_rate
        self.weights_input_hidden += X.T.dot(self.hidden_delta) * learning_rate
        self.biases_output += np.sum(self.output_delta, axis=0) * learning_rate
        self.biases_hidden += np.sum(self.hidden_delta, axis=0) * learning_rate



# Define input data and labels
X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)
y = np.array(([92], [86], [89]), dtype=float)

# Normalize input data
X = X / np.amax(X, axis=0)
y = y / 100  # Assuming you intended to normalize output as well

# Initialize neural network parameters
epoch = 7000
learning_rate = 0.1
input_neurons = 2
hidden_neurons = 3
output_neurons = 1

# Initialize neural network
nn = NeuralNetwork(input_neurons, hidden_neurons, output_neurons)

# Train the neural network
for i in range(epoch):
    output_activation = nn.feedforward(X)
    nn.backpropagation(X, y, output_activation)

# Print results
print("Input:\n", X)
print("Actual Output:\n", y)
print("Predicted Output:\n", output_activation)

#Aim: Assuming a set of documents that need to be classified, use the na√Øve Bayesian Classifier model to perform this task.

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics

msg = pd.read_csv('9b.csv', names=['message', 'label'])
print('Missing values in the dataset:')
msg = msg.dropna()
# Map labels to numerical values
msg['labelnum'] = msg.label.map({'pos': 1, 'neg': 0})
X = msg.message
y = msg.labelnum

# Split the dataset into train and test data
xtrain, xtest, ytrain, ytest = train_test_split(X, y, random_state=1)
print('\nThe total number of Training Data:', len(ytrain))
print('The total number of Test Data:', len(ytest))

# Debugging to check for NaN values in training data
print('\nNaN values in training data:')
print(pd.concat([xtrain, ytrain], axis=1).isnull().sum())

# Remove rows with NaN values in ytrain
missing_index = ytrain.isnull()
xtrain = xtrain[~missing_index]
ytrain = ytrain[~missing_index]

# Initialize CountVectorizer
cv = CountVectorizer(stop_words='english')

# Fit CountVectorizer and transform the data
xtrain_dtm = cv.fit_transform(xtrain)
xtest_dtm = cv.transform(xtest)
# Train Naive Bayes (NB) classifier on training data
clf = MultinomialNB().fit(xtrain_dtm, ytrain)
predicted = clf.predict(xtest_dtm)

# Evaluate the classifier
print('\nAccuracy of the classifier is', metrics.accuracy_score(ytest, predicted))
print('\nConfusion matrix:')
print(metrics.confusion_matrix(ytest, predicted))
print('\nThe value of Precision:', metrics.precision_score(ytest, predicted))
print('The value of Recall:', metrics.recall_score(ytest, predicted))

#Aim: Perform Text pre-processing, Text clustering, classification with Prediction, Test Score and Confusion Matrix.

import re
import numpy as np
import nltk
from sklearn.datasets import load_files
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import joblib

# Download necessary NLTK resources
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Load movie data
movie_data = load_files(‚Äúcontent/text_token ")
X, y = movie_data.data, movie_data.target

# Preprocessing
documents = []
lemmatizer = WordNetLemmatizer()
for sen in range(0, len(X)):
    document = re.sub(r'\W', ' ', str(X[sen]))
    document = re.sub(r'\s+[a-zA-Z]\s+', ' ', document)
    document = re.sub(r'\^[a-zA-Z]\s+', ' ', document)
    document = re.sub(r'\s+', ' ', document, flags=re.I)
    document = re.sub(r'^b\s+', '', document)
    document = document.lower()
    document = document.split()
    document = [lemmatizer.lemmatize(word) for word in document]
    document = ' '.join(document)
    documents.append(document)

# Vectorization
tfidfconverter = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))
X = tfidfconverter.fit_transform(documents).toarray()

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)


# Model training
classifier = RandomForestClassifier(n_estimators=1000, random_state=0)
classifier.fit(X_train, y_train)

# Model evaluation
y_pred = classifier.predict(X_test)
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))
print("Accuracy Score:", accuracy_score(y_test, y_pred))

# Save model
joblib.dump(classifier, 'text_classifier.joblib')

# Load model and predict
model = joblib.load('text_classifier.joblib')
y_pred2 = model.predict(X_test)

# Model evaluation after loading
print("Confusion Matrix after loading:\n", confusion_matrix(y_test, y_pred2))
print("Classification Report after loading:\n", classification_report(y_test, y_pred2))
print("Accuracy Score after loading:", accuracy_score(y_test, y_pred2))

